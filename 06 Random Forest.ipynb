{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from pydataset import data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook options\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Random Forest algorithm (RF) can be used for regression and classification problems and is an ensemble machine learning method. An ensemble method is a way to aggregate less predictive base models to produce a better predictive model. The less predictive base models in the case of RF are decision trees, which tend to overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging stands for bootstrap aggregation which is averaging slightly different versions of a model to improve the prediction. It is designed to improve accuracy by reducing the variance which helps to avoid overfitting. Bagging is a special case of the model averaging approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging can be performed by either subsetting on rows or on features. selecting a random number of samples from the original set can be performed with replacement. In feature bagging the original feature set is randomly sampled and passed onto different trees without replacement since having redundant features makes no sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whereas I created a decision tree classifier in the previous notebook, I will create a Random Forest for regression problems in this notebook. The underlying decision trees are just slightly different. I will create a decision tree class first, so that we can easily use it for the RF later. Note that the function is again recursive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees train by splitting the data into two halves recursively based on certain conditions. If a test set has 10 columns with 10 data points (values) in each column, a total of 10x10 = 100 splits are possible, our task in hand is to find which of these splits is the best for our data. <br>\n",
    "We use the weighted average (score) of the standard deviation of the two halves as the function to measure the quality of a split, which is equal to the mean squared error (MSE)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formula used for standard deviation in this decision tree function is different to the one in which each value is subtracted from the mean and than squared. This is because with this formula, it is possible to loop over the values and accumulate the result, making it way more efficient than to calculate the mean in each iteration. Lower score facilitates lower variance, lower variance facilitates grouping of similar data, which will lead to better predictions.\n",
    "<img src=\"images/Random_forest/standard_deviation.PNG\" alt=\"standard deviation\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_agg(cnt, s1, s2): \n",
    "    # The weighted average of standard deviation of the two halves with number of rows in each half as their weights. \n",
    "    return math.sqrt((s2/cnt) - (s1/cnt)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree():\n",
    "    # As we will now need to split the dataset on rows and on features (by sampling), these parameters become very important.\n",
    "    # We include parameters for max depth and minimal number of samples in a leaf as well,\n",
    "    # more shallow trees results in less overfitting, thereby boosting performance when bagged correctly.\n",
    "    def __init__(self, x, y, n_features, f_idxs, idxs, depth=10, min_leaf=5):\n",
    "        self.x, self.y = x, y\n",
    "        self.idxs, self.f_idxs = idxs, f_idxs\n",
    "        self.min_leaf = min_leaf\n",
    "        self.depth = depth\n",
    "        self.n_features = n_features\n",
    "        self.n, self.c = len(idxs), x.shape[1]\n",
    "        self.val = np.mean(y[idxs]) # Each leaf's prediction is the average of the observations in that leaf\n",
    "        self.score = float('inf') # Starting point, meaning each split will be better than no split at all\n",
    "        self.find_varsplit()\n",
    "        \n",
    "    def find_varsplit(self):\n",
    "        # Loops through every predictor passed in and finds the best split among all\n",
    "        for i in self.f_idxs: self.find_better_split(i)\n",
    "        if self.is_leaf: \n",
    "            print(\"We reached a leaf\")\n",
    "            return # If the node is a leaf already, we should not try to devide further    \n",
    "        # Segregates the column based on its split, lhs is the left side of split, rhs the right side\n",
    "        x = self.split_col # returns the winning column and the values based on the indexes\n",
    "        lhs = np.nonzero(x <= self.split)[0]\n",
    "        rhs = np.nonzero(x > self.split)[0]\n",
    "        # Reshuffle and resample features for both sides of the tree after the split\n",
    "        lf_idxs = np.random.permutation(self.x.shape[1])[:self.n_features]\n",
    "        rf_idxs = np.random.permutation(self.x.shape[1])[:self.n_features]\n",
    "        # Recursively keep fitting a new tree at the root of both sides of the split until is_leaf is not passed anymore\n",
    "        # Subtract 1 from the depth to keep track of how deep the tree is so that is_leaf also maxes at the depth level\n",
    "        print(\"Starting a new branch on the left side\")\n",
    "        self.lhs = DecisionTree(self.x, self.y, self.n_features, lf_idxs, self.idxs[lhs], depth=self.depth-1, min_leaf=self.min_leaf)\n",
    "        print(\"Starting a new branch on the right side\")\n",
    "        self.rhs = DecisionTree(self.x, self.y, self.n_features, rf_idxs, self.idxs[rhs], depth=self.depth-1, min_leaf=self.min_leaf)\n",
    "        \n",
    "    def find_better_split(self, var_idx):\n",
    "        # Finds the best split for a specific column\n",
    "        x, y = self.x.values[self.idxs, var_idx], self.y[self.idxs] \n",
    "        sort_idx = np.argsort(x) # Returns indexes of the column sorted by its values\n",
    "        sort_y, sort_x = y[sort_idx], x[sort_idx] # Sorts the x and y value based on the sorted indexes\n",
    "        \n",
    "        # The number of rows, the sum of the target var and the sum of the target squared (for the right side of sorted split)\n",
    "        rhs_cnt, rhs_sum, rhs_sum2 = self.n, sort_y.sum(), (sort_y**2).sum()\n",
    "        lhs_cnt, lhs_sum, lhs_sum2 = 0, 0., 0. # Initiate values for the left side of sorted split\n",
    "        \n",
    "        for i in range(0, self.n-self.min_leaf-1): \n",
    "            # Loop over the possible splits starting left in the sorted list, \n",
    "            # ending at the minimal number of samples from the right side trying every possible split\n",
    "            # while keeping in mind the minimum number of samples in a split.\n",
    "            # Since we initiate values at 0 and accumulate in each loop it is not possible to start left on min_leaf\n",
    "            xi, yi = sort_x[i], sort_y[i]\n",
    "            lhs_cnt += 1; rhs_cnt -= 1\n",
    "            lhs_sum += yi; rhs_sum -= yi\n",
    "            lhs_sum2 += yi**2; rhs_sum2 -= yi**2\n",
    "            # If on the left side there is not yet enough samples to reach min_leaf or if the next value is the same,\n",
    "            # and so the split would be the same, than continue accumulating\n",
    "            if i < self.min_leaf or xi == sort_x[i+1]:\n",
    "                continue\n",
    "            \n",
    "            # Calculate the score at each split\n",
    "            # If a new best score is obtained, save the var indx, score and specified split to self\n",
    "            lhs_std = std_agg(lhs_cnt, lhs_sum, lhs_sum2)\n",
    "            rhs_std = std_agg(rhs_cnt, rhs_sum, rhs_sum2)\n",
    "            curr_score = lhs_std*lhs_cnt + rhs_std*rhs_cnt\n",
    "            \n",
    "            if curr_score < self.score: \n",
    "                self.var_idx, self.score, self.split = var_idx, curr_score, xi\n",
    "                print(\"Current split updated on variable {} and split {} with score {}\".format(self.var_idx, self.split, self.score))\n",
    "        \n",
    "        \n",
    "    # Properties for nested functions make the code concise with the rest of the syntax used in the Class object\n",
    "    @property\n",
    "    def split_name(self): \n",
    "        # Return the column name rather than the index of the splitcol\n",
    "        return self.x.columns[self.var_idx] \n",
    "        \n",
    "    @property\n",
    "    def split_col(self):\n",
    "        # Splits the idxs based on the splitcol and splitvalue\n",
    "        return self.x.values[self.idxs, self.var_idx] \n",
    "        \n",
    "    @property\n",
    "    def is_leaf(self):\n",
    "        # Function to check if the node is a leaf, i.e. max depth is reached \n",
    "        # If the decision tree is recurrently called, the score will be reset to 'inf', if no new split can be made \n",
    "        # than 'inf' must stop the call of a new branche in the decision tree\n",
    "        return self.score == float('inf') or self.depth <= 0 \n",
    "\n",
    "    \n",
    "    def predict(self, x):\n",
    "        # Performs prediction for each row, returning a vector of preds\n",
    "        return np.array([self.predict_row(xi) for xi in x])\n",
    "\n",
    "    def predict_row(self, xi):\n",
    "        # Goes through the tree from top to bottom until a leaf is reached,\n",
    "        # follows the splits according to input data and teturns the average of the leaf\n",
    "        if self.is_leaf: return self.val\n",
    "        t = self.lhs if xi[self.var_idx] <= self.split else self.rhs\n",
    "        return t.predict_row(xi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will try out the decision tree to see how exactly the function works before implementing the Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cyl</th>\n",
       "      <th>disp</th>\n",
       "      <th>hp</th>\n",
       "      <th>drat</th>\n",
       "      <th>wt</th>\n",
       "      <th>qsec</th>\n",
       "      <th>vs</th>\n",
       "      <th>am</th>\n",
       "      <th>gear</th>\n",
       "      <th>carb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>32.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>32.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>20.090625</td>\n",
       "      <td>6.187500</td>\n",
       "      <td>230.721875</td>\n",
       "      <td>146.687500</td>\n",
       "      <td>3.596563</td>\n",
       "      <td>3.217250</td>\n",
       "      <td>17.848750</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.406250</td>\n",
       "      <td>3.687500</td>\n",
       "      <td>2.8125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.026948</td>\n",
       "      <td>1.785922</td>\n",
       "      <td>123.938694</td>\n",
       "      <td>68.562868</td>\n",
       "      <td>0.534679</td>\n",
       "      <td>0.978457</td>\n",
       "      <td>1.786943</td>\n",
       "      <td>0.504016</td>\n",
       "      <td>0.498991</td>\n",
       "      <td>0.737804</td>\n",
       "      <td>1.6152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>10.400000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>71.100000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>2.760000</td>\n",
       "      <td>1.513000</td>\n",
       "      <td>14.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>15.425000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>120.825000</td>\n",
       "      <td>96.500000</td>\n",
       "      <td>3.080000</td>\n",
       "      <td>2.581250</td>\n",
       "      <td>16.892500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>19.200000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>196.300000</td>\n",
       "      <td>123.000000</td>\n",
       "      <td>3.695000</td>\n",
       "      <td>3.325000</td>\n",
       "      <td>17.710000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>22.800000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>326.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>3.920000</td>\n",
       "      <td>3.610000</td>\n",
       "      <td>18.900000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>33.900000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>472.000000</td>\n",
       "      <td>335.000000</td>\n",
       "      <td>4.930000</td>\n",
       "      <td>5.424000</td>\n",
       "      <td>22.900000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>8.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             mpg        cyl        disp          hp       drat         wt  \\\n",
       "count  32.000000  32.000000   32.000000   32.000000  32.000000  32.000000   \n",
       "mean   20.090625   6.187500  230.721875  146.687500   3.596563   3.217250   \n",
       "std     6.026948   1.785922  123.938694   68.562868   0.534679   0.978457   \n",
       "min    10.400000   4.000000   71.100000   52.000000   2.760000   1.513000   \n",
       "25%    15.425000   4.000000  120.825000   96.500000   3.080000   2.581250   \n",
       "50%    19.200000   6.000000  196.300000  123.000000   3.695000   3.325000   \n",
       "75%    22.800000   8.000000  326.000000  180.000000   3.920000   3.610000   \n",
       "max    33.900000   8.000000  472.000000  335.000000   4.930000   5.424000   \n",
       "\n",
       "            qsec         vs         am       gear     carb  \n",
       "count  32.000000  32.000000  32.000000  32.000000  32.0000  \n",
       "mean   17.848750   0.437500   0.406250   3.687500   2.8125  \n",
       "std     1.786943   0.504016   0.498991   0.737804   1.6152  \n",
       "min    14.500000   0.000000   0.000000   3.000000   1.0000  \n",
       "25%    16.892500   0.000000   0.000000   3.000000   2.0000  \n",
       "50%    17.710000   0.000000   0.000000   4.000000   2.0000  \n",
       "75%    18.900000   1.000000   1.000000   4.000000   4.0000  \n",
       "max    22.900000   1.000000   1.000000   5.000000   8.0000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use a very small but clear dataset\n",
    "df = data('mtcars')\n",
    "df = df.reset_index(drop=True)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 11)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cyl</th>\n",
       "      <th>disp</th>\n",
       "      <th>hp</th>\n",
       "      <th>drat</th>\n",
       "      <th>wt</th>\n",
       "      <th>qsec</th>\n",
       "      <th>vs</th>\n",
       "      <th>am</th>\n",
       "      <th>gear</th>\n",
       "      <th>carb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21.0</td>\n",
       "      <td>6</td>\n",
       "      <td>160.0</td>\n",
       "      <td>110</td>\n",
       "      <td>3.90</td>\n",
       "      <td>2.620</td>\n",
       "      <td>16.46</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21.0</td>\n",
       "      <td>6</td>\n",
       "      <td>160.0</td>\n",
       "      <td>110</td>\n",
       "      <td>3.90</td>\n",
       "      <td>2.875</td>\n",
       "      <td>17.02</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22.8</td>\n",
       "      <td>4</td>\n",
       "      <td>108.0</td>\n",
       "      <td>93</td>\n",
       "      <td>3.85</td>\n",
       "      <td>2.320</td>\n",
       "      <td>18.61</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21.4</td>\n",
       "      <td>6</td>\n",
       "      <td>258.0</td>\n",
       "      <td>110</td>\n",
       "      <td>3.08</td>\n",
       "      <td>3.215</td>\n",
       "      <td>19.44</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18.7</td>\n",
       "      <td>8</td>\n",
       "      <td>360.0</td>\n",
       "      <td>175</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.440</td>\n",
       "      <td>17.02</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mpg  cyl   disp   hp  drat     wt   qsec  vs  am  gear  carb\n",
       "0  21.0    6  160.0  110  3.90  2.620  16.46   0   1     4     4\n",
       "1  21.0    6  160.0  110  3.90  2.875  17.02   0   1     4     4\n",
       "2  22.8    4  108.0   93  3.85  2.320  18.61   1   1     4     1\n",
       "3  21.4    6  258.0  110  3.08  3.215  19.44   1   0     3     1\n",
       "4  18.7    8  360.0  175  3.15  3.440  17.02   0   0     3     2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview of the data at hand\n",
    "df.shape\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate the predictors and target variable\n",
    "X = df.iloc[:,1:]\n",
    "Y = np.array(df.mpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we created the Decision tree for the random forest, we're supposed to give indexes for rows and features\n",
    "idxs = np.random.permutation(len(Y))[:25]\n",
    "f_idxs = np.random.permutation(X.shape[1])[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current split updated on variable 9 and split 1.0 with score 130.78321500193414\n",
      "Current split updated on variable 9 and split 2.0 with score 104.77541590148145\n",
      "Current split updated on variable 4 and split 2.2 with score 85.26678125819004\n",
      "Starting a new branch on the left side\n",
      "We reached a leaf\n",
      "Starting a new branch on the right side\n",
      "Current split updated on variable 5 and split 15.41 with score 71.09030379645372\n",
      "Current split updated on variable 5 and split 17.98 with score 66.55512991830639\n",
      "Current split updated on variable 5 and split 18.0 with score 61.80563409095271\n",
      "Current split updated on variable 2 and split 97.0 with score 61.62434463562479\n",
      "Current split updated on variable 2 and split 109.0 with score 61.19029109775741\n",
      "Current split updated on variable 2 and split 110.0 with score 52.03370787120123\n",
      "Current split updated on variable 2 and split 123.0 with score 51.4893137934705\n",
      "Current split updated on variable 2 and split 175.0 with score 49.07025875739076\n",
      "Starting a new branch on the left side\n",
      "Current split updated on variable 4 and split 2.62 with score 24.1990085687965\n",
      "Current split updated on variable 4 and split 2.78 with score 23.572319073014665\n",
      "Current split updated on variable 4 and split 2.875 with score 22.084824882407318\n",
      "Current split updated on variable 4 and split 3.19 with score 17.081325037134512\n",
      "Starting a new branch on the left side\n",
      "Current split updated on variable 3 and split 3.7 with score 8.768344009973319\n",
      "Current split updated on variable 3 and split 3.85 with score 7.464960749214867\n",
      "Current split updated on variable 9 and split 2.0 with score 6.69954747217964\n",
      "Current split updated on variable 2 and split 97.0 with score 6.125188905530683\n",
      "We reached a leaf\n",
      "Starting a new branch on the right side\n",
      "We reached a leaf\n",
      "Starting a new branch on the right side\n",
      "Current split updated on variable 1 and split 275.8 with score 15.522103995182245\n",
      "Current split updated on variable 1 and split 301.0 with score 14.759488009455689\n",
      "Current split updated on variable 1 and split 351.0 with score 13.559085956540361\n",
      "Starting a new branch on the left side\n",
      "Current split updated on variable 1 and split 275.8 with score 5.707996734335396\n",
      "Current split updated on variable 2 and split 180.0 with score 5.707996734335272\n",
      "We reached a leaf\n",
      "Starting a new branch on the right side\n",
      "We reached a leaf\n"
     ]
    }
   ],
   "source": [
    "# Now we can fit the Decision Tree\n",
    "# Due to the print statements we can follow its inner workings\n",
    "DT = DecisionTree(x = X, y = Y, n_features=8, f_idxs=f_idxs, idxs=idxs, depth=3, min_leaf=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The initial best split is on variable 4 which is wt, with split value 2.2.\n"
     ]
    }
   ],
   "source": [
    "print(\"The initial best split is on variable {} which is {}, with split value {}.\".format(DT.var_idx, DT.split_name, DT.split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.DecisionTree at 0x20428a8fd30>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<__main__.DecisionTree at 0x20428a8f898>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After the initial split we create two new decision trees\n",
    "DT.lhs\n",
    "DT.rhs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best split for all data on the right side of the initial split is on variable 2 which is hp, with split value 175.0.\n"
     ]
    }
   ],
   "source": [
    "# Due to the small dataset the left side of the tree was immediately a leaf\n",
    "print(\"The best split for all data on the right side of the initial split is on variable {} which is {}, with split value {}.\"\n",
    "     .format(DT.rhs.var_idx, DT.rhs.split_name, DT.rhs.split))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the recursive functions, we first continue on the left side of the tree working its way down, until a leaf is reached and we can go back up one step. The tree thereby builts itself from left to right basically, although starting at the top and working its way down. <br>\n",
    "The next branche can be reached with e.g. DT.lhs.rhs... or DT.lhs.lhs... However due to the random splits of rows and features, the tree could look different every time. This is exactly the idea of the random forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predict function is also a recursive function. It checks at each split, starting at the root, whether the node is a leaf. If so, it returns the average of the train samples in that leaf. If not it compares the input observation with the split in that node, and follows the tree accordingly until a leaf is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([21.68571429, 21.68571429, 21.68571429, 17.9       , 17.9       ,\n",
       "       17.9       , 11.83333333, 21.68571429, 21.68571429, 17.9       ,\n",
       "       17.9       , 15.5       , 15.5       , 15.5       , 11.83333333,\n",
       "       11.83333333, 11.83333333, 30.125     , 30.125     , 30.125     ,\n",
       "       21.68571429, 17.9       , 17.9       , 15.5       , 17.9       ,\n",
       "       30.125     , 30.125     , 30.125     , 15.5       , 21.68571429,\n",
       "       15.5       , 21.68571429])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since we fitted the model on a large part of this data, it is not weird that the predictions are close to the actuals already\n",
    "DT.predict(np.array(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=3, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=2,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([20.225     , 20.225     , 23.6       , 17.02857143, 20.225     ,\n",
       "       17.02857143, 14.7       , 23.6       , 23.6       , 17.02857143,\n",
       "       17.02857143, 17.02857143, 17.02857143, 17.02857143, 10.4       ,\n",
       "       10.4       , 14.7       , 31.4       , 31.4       , 31.4       ,\n",
       "       21.45      , 20.225     , 17.02857143, 14.7       , 20.225     ,\n",
       "       28.85      , 23.6       , 28.85      , 14.7       , 20.225     ,\n",
       "       14.7       , 21.45      ])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do a check with sklearn to see if we are on the right track\n",
    "# Due to our random permutation the results might be different, but by the looks of it they are quite similar\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "skl_DT = DecisionTreeRegressor(max_depth=3, min_samples_leaf=2)\n",
    "skl_DT.fit(np.array(X)[idxs], np.array(Y)[idxs])\n",
    "skl_DT.predict(np.array(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the decision tree regressor is in place, it is rather easy to fit a random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest():\n",
    "    def __init__(self, x, y, n_trees, n_features, sample_sz, random_seed=0, depth=10, min_leaf=5):\n",
    "        np.random.seed(random_seed) # set seed to be able to reproduce results\n",
    "        self.x, self.y = x, y \n",
    "        self.n_features, self.sample_sz, self.depth, self.min_leaf = n_features, sample_sz, depth, min_leaf\n",
    "        # All we do is create a bunch of decision trees and store them in an array\n",
    "        self.trees = [self.create_tree() for i in range(n_trees)]\n",
    "\n",
    "    def create_tree(self):\n",
    "        # For every Decision tree a different random set of rows and features is used\n",
    "        idxs = np.random.permutation(len(self.y))[:self.sample_sz]\n",
    "        f_idxs = np.random.permutation(self.x.shape[1])[:self.n_features]\n",
    "        return DecisionTree(self.x.iloc[idxs], \n",
    "                            self.y[idxs], \n",
    "                            self.n_features, \n",
    "                            f_idxs,\n",
    "                            idxs = np.array(range(self.sample_sz)), \n",
    "                            depth = self.depth, \n",
    "                            min_leaf = self.min_leaf)\n",
    "        \n",
    "    def predict(self, x):\n",
    "        # Once we have all the slightly different decision trees, we call the predict method on each and average the results\n",
    "        return np.mean([t.predict(x) for t in self.trees], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets make the decision tree again without the print statements, as we are now going to call it many times\n",
    "class DecisionTree():\n",
    "    def __init__(self, x, y, n_features, f_idxs, idxs, depth=10, min_leaf=5):\n",
    "        self.x, self.y = x, y\n",
    "        self.idxs, self.f_idxs = idxs, f_idxs\n",
    "        self.min_leaf = min_leaf\n",
    "        self.depth = depth\n",
    "        self.n_features = n_features\n",
    "        self.n, self.c = len(idxs), x.shape[1]\n",
    "        self.val = np.mean(y[idxs]) \n",
    "        self.score = float('inf') \n",
    "        self.find_varsplit()\n",
    "        \n",
    "    def find_varsplit(self):\n",
    "        for i in self.f_idxs: self.find_better_split(i)\n",
    "        if self.is_leaf: return \n",
    "        x = self.split_col   \n",
    "        lhs = np.nonzero(x <= self.split)[0]\n",
    "        rhs = np.nonzero(x > self.split)[0]\n",
    "        lf_idxs = np.random.permutation(self.x.shape[1])[:self.n_features]\n",
    "        rf_idxs = np.random.permutation(self.x.shape[1])[:self.n_features]\n",
    "        self.lhs = DecisionTree(self.x, self.y, self.n_features, lf_idxs, self.idxs[lhs], depth=self.depth-1, min_leaf=self.min_leaf)\n",
    "        self.rhs = DecisionTree(self.x, self.y, self.n_features, rf_idxs, self.idxs[rhs], depth=self.depth-1, min_leaf=self.min_leaf)\n",
    "        \n",
    "    def find_better_split(self, var_idx):\n",
    "        x, y = self.x.values[self.idxs, var_idx], self.y[self.idxs] \n",
    "        sort_idx = np.argsort(x) \n",
    "        sort_y, sort_x = y[sort_idx], x[sort_idx] \n",
    "        rhs_cnt, rhs_sum, rhs_sum2 = self.n, sort_y.sum(), (sort_y**2).sum()\n",
    "        lhs_cnt, lhs_sum, lhs_sum2 = 0, 0., 0. \n",
    "        \n",
    "        for i in range(0, self.n-self.min_leaf-1): \n",
    "            xi, yi = sort_x[i], sort_y[i]\n",
    "            lhs_cnt += 1; rhs_cnt -= 1\n",
    "            lhs_sum += yi; rhs_sum -= yi\n",
    "            lhs_sum2 += yi**2; rhs_sum2 -= yi**2\n",
    "            if i < self.min_leaf or xi == sort_x[i+1]:\n",
    "                continue\n",
    "            \n",
    "            lhs_std = std_agg(lhs_cnt, lhs_sum, lhs_sum2)\n",
    "            rhs_std = std_agg(rhs_cnt, rhs_sum, rhs_sum2)\n",
    "            curr_score = lhs_std*lhs_cnt + rhs_std*rhs_cnt\n",
    "            if curr_score < self.score: \n",
    "                self.var_idx, self.score, self.split = var_idx, curr_score, xi\n",
    "        \n",
    "    @property\n",
    "    def split_name(self): \n",
    "        return self.x.columns[self.var_idx] \n",
    "        \n",
    "    @property\n",
    "    def split_col(self):\n",
    "        return self.x.values[self.idxs, self.var_idx] \n",
    "        \n",
    "    @property\n",
    "    def is_leaf(self):\n",
    "        return self.score == float('inf') or self.depth <= 0 \n",
    "\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return np.array([self.predict_row(xi) for xi in x])\n",
    "\n",
    "    def predict_row(self, xi):\n",
    "        if self.is_leaf: return self.val\n",
    "        t = self.lhs if xi[self.var_idx] <= self.split else self.rhs\n",
    "        return t.predict_row(xi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate our variables again in order to fit the Random Forest\n",
    "X = df.iloc[:,1:]\n",
    "Y = np.array(df.mpg)\n",
    "n_trees = 10\n",
    "n_features = 8\n",
    "sample_sz = 24\n",
    "min_leaf = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And actually fit the model to the 'traindata'\n",
    "RF = RandomForest(x = X, y = Y, n_trees=n_trees, n_features=n_features, sample_sz=sample_sz, min_leaf=min_leaf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([21.13333333, 21.13333333, 22.49683333, 20.014     , 17.6915    ,\n",
       "       19.5065    , 14.346     , 22.04016667, 22.49683333, 18.76316667,\n",
       "       18.76316667, 16.27283333, 16.27283333, 16.27283333, 12.944     ,\n",
       "       12.944     , 12.944     , 30.13933333, 30.87733333, 30.87733333,\n",
       "       22.09016667, 17.2195    , 17.2915    , 13.79933333, 17.6915    ,\n",
       "       30.01066667, 28.436     , 30.01066667, 15.27266667, 20.18466667,\n",
       "       14.996     , 21.9235    ])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using the predict method as we did with the Decision Tree\n",
    "RF.predict(np.array(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.DecisionTree at 0x20429bd6da0>,\n",
       " <__main__.DecisionTree at 0x20429bd6f60>,\n",
       " <__main__.DecisionTree at 0x20429bd6be0>,\n",
       " <__main__.DecisionTree at 0x20429be34a8>,\n",
       " <__main__.DecisionTree at 0x20429be37b8>,\n",
       " <__main__.DecisionTree at 0x20429be3940>,\n",
       " <__main__.DecisionTree at 0x20429be3c88>,\n",
       " <__main__.DecisionTree at 0x20428d207b8>,\n",
       " <__main__.DecisionTree at 0x20428d20828>,\n",
       " <__main__.DecisionTree at 0x20429bf3518>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can find that the Random Forest object now indeed holds 10 decision trees\n",
    "RF.trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So we can actually use each individual Decision Tree to make predictions or check how it is build up\n",
    "pred_list = []\n",
    "for tree in RF.trees:\n",
    "    pred_list.append(tree.predict(np.array(X))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20.7,\n",
       " 21.3,\n",
       " 22.22,\n",
       " 21.54,\n",
       " 20.14,\n",
       " 20.566666666666666,\n",
       " 21.3,\n",
       " 22.3,\n",
       " 20.7,\n",
       " 20.566666666666666]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The first observation received a prediction of 21.1333..\n",
    "# Which is made up out of the 10 predictions from all the trees\n",
    "pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.133333333333333"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As we see this indeed lines up with the average prediction that the Random Forest gave us\n",
    "np.mean(pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used resources\n",
    "#https://towardsdatascience.com/random-forests-and-decision-trees-from-scratch-in-python-3e4fa5ae4249\n",
    "#https://www.analyticsvidhya.com/blog/2018/12/building-a-random-forest-from-scratch-understanding-real-world-data-products-ml-for-programmers-part-3/\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n",
    "#https://towardsdatascience.com/de-coding-random-forests-82d4dcbb91a1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
